#!/bin/bash

# Colors for output
RED="\e[31m"
GREEN="\e[32m"
YELLOW="\e[33m"
RESET="\e[0m"

# ┌─────────────────────────────────────────────┐
# │                                             │
# |         RECON TOOL - v1.0   @M9nx           │
# │           Subdomain & Web Recon             |
# └─────────────────────────────────────────────┘
banner() {
    cat <<"EOF"

██████╗ ███████╗ ██████╗ ██████╗  ██████╗ ███╗   ██╗
██╔══██╗██╔════╝██╔════╝ ██╔══██╗██╔═══██╗████╗  ██║
██████╔╝█████╗  ██║  ███╗██████╔╝██║   ██║██╔██╗ ██║
██╔═══╝ ██╔══╝  ██║   ██║██╔═══╝ ██║   ██║██║╚██╗██║
██║     ███████╗╚██████╔╝██║     ╚██████╔╝██║ ╚████║
╚═╝     ╚══════╝ ╚═════╝ ╚═╝      ╚═════╝ ╚═╝  ╚═══╝

        [ PEGPON - v1.1   @M9nx ]
EOF
}
# Entry point
banner
run_recon_for_domain() {
    domain="$1"
    echo "Running recon for: $domain"
    output_dir="recon/$domain"
    mkdir -p "$output_dir"
    log_file="$output_dir/recon.log"

    log() {
        echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$log_file"
    }

    # Passive Subdomain Enumeration
    run_passive_recon() {
        log "Running Passive Recon..."

        # Export Chaos API Key
        export CHAOS_KEY="YOUR_TOKEN"  # put ur token
        export GITHUB_TOKEN="YOUR_TOKEN" # put ur token
        export SECURITYTRAILS_KEY="YOUR_KEY"  # put ur key
        export NETLAS_API_KEY="YOUR_KEY"     # put ur key
        export ZOOMEYE_API_KEY="YOUR_KEY" # put ur key

        # Make sure output dir exists
        mkdir -p "$output_dir"

        # Subfinder
        log "Using Subfinder..."
        subfinder -d $domain -o $output_dir/subfinder.txt

        # Assetfinder
        log "Using Assetfinder..."
        assetfinder --subs-only $domain >$output_dir/assetfinder.txt

        # Sublist3r
        log "Using Sublist3r..."
        sublist3r -d $domain -o $output_dir/sublist3r.txt

        # crt.sh via curl
        log "Using crt.sh..."
        curl -s "https://crt.sh/?q=%25.$domain&output=json" |
            jq -r '.[].name_value' |
            sed 's/\*\.//g' |
            grep -iE "([a-z0-9_\-]+\.)+$domain$" |
            sort -u >$output_dir/crtsh.txt

        # ShrewdEye scraping (static HTML parsing)  -->new
        log "Using ShrewdEye..."
        curl -s "https://shrewdeye.app/?q=$domain" |
            grep -oP '([a-zA-Z0-9_-]+\.)+'"$domain" |
            sort -u >"$output_dir/shrewdeye.txt"

        # SecurityTrails API (requires SECURITYTRAILS_KEY)  -->new
        if [[ -z "$SECURITYTRAILS_KEY" ]]; then
            log "Skipping SecurityTrails: SECURITYTRAILS_KEY not set."
        else
            log "Using SecurityTrails API..."
            curl -s -H "APIKEY: $SECURITYTRAILS_KEY" \
                "https://api.securitytrails.com/v1/domain/$domain/subdomains" |
                jq -r '.subdomains[]' |
                sed "s|$|.$domain|" |
                sort -u >"$output_dir/securitytrails.txt"
        fi

        # Findomain
        log "Using Findomain..."
        findomain --target $domain --quiet --output
        mv "$domain.txt" "$output_dir/findomain.txt"

        # Chaos
        log "Using Chaos dataset..."
        chaos -d $domain -o $output_dir/chaos.txt

        # GitHub Subdomains (requires GitHub token set in env var)
        if [ ! -z "$GITHUB_TOKEN" ]; then
            log "Using GitHub-subdomains..."
            github-subdomains -d $domain -t $GITHUB_TOKEN -o $output_dir/github.txt
        else
            log "Skipping GitHub-subdomains: GITHUB_TOKEN not set."
        fi

        # Netlas.io (requires API key) #-->new
        if [[ -n "$NETLAS_API_KEY" ]]; then
            log "Using Netlas.io..."
            curl -s "https://app.netlas.io/api/domains?q=domain:$domain" \
                -H "X-API-Key: $NETLAS_API_KEY" |
                jq -r '.items[].name' |
                grep "$domain" |
                sort -u >"$output_dir/netlas.txt"
        else
            log "Skipping Netlas.io: API key not set."
        fi

        # Bufferover.run #-->new
        log "Using Bufferover..."
        curl -s "https://dns.bufferover.run/dns?q=.$domain" |
            jq -r '.FDNS_A[]?' |
            cut -d',' -f2 |
            grep "$domain" |
            sort -u >"$output_dir/bufferover.txt"

        # Zoomeye (requires API key) #-->new
        if [[ -n "$ZOOMEYE_API_KEY" ]]; then
            log "Using Zoomeye..."
            curl -s -H "API-KEY: $ZOOMEYE_API_KEY" \
                "https://api.zoomeye.org/domain/search?q=$domain" |
                jq -r '.matches[].name' |
                grep "$domain" |
                sort -u >"$output_dir/zoomeye.txt"
        else
            log "Skipping Zoomeye: API key not set."
        fi

        # LeakIX #-->new
        log "Using LeakIX..."
        curl -s "https://leakix.net/domain/$domain" |
            grep -oP '([a-zA-Z0-9_-]+\.)+'"$domain" |
            sort -u >"$output_dir/leakix.txt"

        # Combine & deduplicate all results into passive_subdomains.txt
        log "Combining and sorting results..."
        cat $output_dir/*.txt | sort -u >$output_dir/passive_subdomains.txt

        # Clean up all temp files except passive_subdomains.txt
        log "Cleaning up temporary files..."
        find $output_dir -type f -name '*.txt' ! -name 'passive_subdomains.txt' -delete

        log "Passive recon completed. Results saved to $output_dir/passive_subdomains.txt"
    }

    # Active Subdomain Enumeration via FFUF Subdomain Fuzzing
    run_active_recon() {
        log "Running FFUF Subdomain Fuzzing..."
        wordlist="/usr/share/dirb/wordlists/subdomains-top1million-110000.txt" # --> update this path to ur path

        # Create output directory if not exists
        mkdir -p "$output_dir"

        # Temporary file to collect all subs
        tmp_file="$output_dir/tmp_active_subdomains.txt"
        >"$tmp_file"

        # Level 1
        log "[Level 1] Fuzzing FUZZ.$domain"
        ffuf -w $wordlist -u "https://FUZZ.$domain" -H "Host: FUZZ.$domain" -fs 0 -s -o "$output_dir/ffuf_lvl1.json" -of json
        level1_results=$(jq -r '.results[].input.FUZZ' "$output_dir/ffuf_lvl1.json")

        if [[ -n "$level1_results" ]]; then
            echo "$level1_results" | sed "s|$|.$domain|" >>"$tmp_file"
            rm "$output_dir/ffuf_lvl1.json"

            # Level 2
            for sub1 in $level1_results; do
                log "[Level 2] Fuzzing FUZZ.$sub1.$domain"
                ffuf -w $wordlist -u "https://FUZZ.$sub1.$domain" -H "Host: FUZZ.$sub1.$domain" -fs 0 -s -o "$output_dir/ffuf_lvl2_${sub1}.json" -of json
                level2_results=$(jq -r '.results[].input.FUZZ' "$output_dir/ffuf_lvl2_${sub1}.json")

                if [[ -n "$level2_results" ]]; then
                    echo "$level2_results" | sed "s|$|.$sub1.$domain|" >>"$tmp_file"

                    # Level 3
                    for sub2 in $level2_results; do
                        log "[Level 3] Fuzzing FUZZ.$sub2.$sub1.$domain"
                        ffuf -w $wordlist -u "https://FUZZ.$sub2.$sub1.$domain" -H "Host: FUZZ.$sub2.$sub1.$domain" -fs 0 -s -o "$output_dir/ffuf_lvl3_${sub2}_${sub1}.json" -of json
                        level3_results=$(jq -r '.results[].input.FUZZ' "$output_dir/ffuf_lvl3_${sub2}_${sub1}.json")

                        if [[ -n "$level3_results" ]]; then
                            echo "$level3_results" | sed "s|$|.$sub2.$sub1.$domain|" >>"$tmp_file"
                        fi
                        rm "$output_dir/ffuf_lvl3_${sub2}_${sub1}.json"
                    done
                fi
                rm "$output_dir/ffuf_lvl2_${sub1}.json"
            done
        else
            log "No results found at Level 1. Halting recursion."
            rm "$output_dir/ffuf_lvl1.json"
        fi

        # Remove duplicates and save final results
        sort -u "$tmp_file" >"$output_dir/active_subdomains.txt"

        # Clean up temp file
        rm "$tmp_file"

        log "Active recon completed. Results saved to $output_dir/active_subdomains.txt"
    }

    # Fuzzing Passive Subdomins with ffuf
    run_ffuf_passive() {
        log "Fuzzing passive subdomains with ffuf..."
        wordlist="/usr/share/dirb/wordlists/subdomains-top1million-110000.txt" # --> update this path to ur path
        passive_file="$output_dir/passive_subdomains.txt"

        mkdir -p "$output_dir"

        tmp_file="$output_dir/tmp_active_passive.txt"
        >"$tmp_file"

        # Loop over each passive subdomain line by line
        while read -r base; do
            current_subs="$base"

            for level in {1..3}; do
                next_subs=""

                # Loop over current subs (split by space)
                for sub in $current_subs; do
                    log "[Level $level] Fuzzing FUZZ.$sub"

                    ffuf -w "$wordlist" -u "https://FUZZ.$sub" -H "Host: FUZZ.$sub" -fs 0 -s -o "$output_dir/ffuf_${sub}_lvl${level}.json" -of json
                    fuzz_results=$(jq -r '.results[].input.FUZZ' "$output_dir/ffuf_${sub}_lvl${level}.json")

                    if [[ -n "$fuzz_results" ]]; then
                        # Add discovered subs to temp file with full path
                        echo "$fuzz_results" | sed "s|$|.$sub|" >>"$tmp_file"

                        # Prepare next level subs, space-separated
                        next_subs="$next_subs $(echo "$fuzz_results" | sed "s|$|.$sub|")"
                    fi

                    rm "$output_dir/ffuf_${sub}_lvl${level}.json"
                done

                # Update current_subs for the next level (space-separated list)
                current_subs="$next_subs"
            done
        done <"$passive_file"

        # Sort, remove duplicates, and save final result
        sort -u "$tmp_file" >"$output_dir/active_passive_subdomains.txt"
        rm "$tmp_file"

        log "Passive fuzzing completed. Results saved to $output_dir/active_passive_subdomains.txt"
    }

    # HTTP Probing 1
    run_httpx1() {
        log "Probing live subdomains with Httpx..."

        combined_file="$output_dir/combined_subdomains.txt"
        cat "$output_dir/active_subdomains.txt" "$output_dir/passive_subdomains.txt" "$output_dir/active_passive_subdomains.txt" | sort -u >"$combined_file"

        cat "$combined_file" | httpx -o "$output_dir/final_live_subdomains.txt"

        rm "$combined_file"

        log "Httpx probing completed. Results saved to $output_dir/final_live_subdomains.txt"
    }

    #run_dirsearch_batch
    run_dirsearch_batch() {
        log "Running Dirsearch on all live subdomains..."

        input_file="$output_dir/final_live_subdomains.txt"
        output_file="$output_dir/dirsearch_valid_urls.txt"

        if [[ ! -f "$input_file" ]]; then
            echo -e "${RED}Error:${RESET} $input_file not found."
            return
        fi

        dirsearch_exts="conf,config,bak,backup,swp,old,db,sql,asp,aspx,asp~,py,py~,rb,rb~,php,php~,bkp,cache,cgi,csv,html,inc,jar,js,json,jsp,jsp~,lock,log,rar,sql.gz,sql.tar.gz,sql~,swp~,tar,tar.bz2,tar.gz,txt,wadl,zip,xml"

        python3 dirsearch/dirsearch.py \
            -l "$input_file" \
            -e "$dirsearch_exts" \
            --plain-text-report "$output_file"

        log "Dirsearch scan completed. Results saved to $output_file"
    }

    # Crawling with Katana
    run_katana() {
        log "Running Katana for web crawling..."
        katana -list "$output_dir/final_live_subdomains.txt" -depth 3 -o "$output_dir/katana_urls.txt"
    }

    # Extract URLs from Wayback Machine
    run_waybackurls() {
        log "Extracting URLs from Wayback Machine..."
        cat "$output_dir/final_live_subdomains.txt" | waybackurls >"$output_dir/wayback_urls.txt"
    }

    # Run GAU to fetch URLs
    run_gau() {
        log "Running GAU for historical URL fetching..."
        combined_output="$output_dir/gau_all_urls.txt"
        >"$combined_output"

        while read -r subdomain; do
            log "Fetching URLs for $subdomain with GAU..."
            gau "$subdomain" >>"$combined_output"
        done <"$output_dir/final_live_subdomains.txt"

        sort -u "$combined_output" -o "$combined_output"

        log "GAU completed. All URLs saved to $combined_output"
    }

    # Combine all URLs into all_urls.txt
    combine_all_urls() {
        log "Combining Katana, Waybackurls, and GAU outputs..."
        cat "$output_dir/katana_urls.txt" \
            "$output_dir/wayback_urls.txt" \
            "$output_dir/gau_all_urls.txt" |
            sort -u >"$output_dir/all_urls.txt"

        log "All URLs combined into $output_dir/all_urls.txt"
    }

    # HTTP Probing 2
    run_httpx2() {
        log "all_urls.txt with Httpx..."

        cat "$output_dir/all_urls.txt" | httpx -o "$output_dir/final_all_urls.txt"

        log "Httpx probing completed. Results saved to $output_dir/final_all_urls.txt"
    }

    # Filtering URLs by extensions
    filter_urls() {
        log "Filtering URLs by file extensions..."

        # Create the directory if it doesn't exist
        mkdir -p "$output_dir/urls"

        grep -iE '\.js([/?].*)?$' "$output_dir/all_urls.txt" >"$output_dir/urls/js_urls.txt"
        grep -iE '\.php([/?].*)?$' "$output_dir/all_urls.txt" >"$output_dir/urls/php_urls.txt"
        grep -iE '\.aspx([/?].*)?$' "$output_dir/all_urls.txt" >"$output_dir/urls/aspx_urls.txt"
        grep -iE '\.jsp([/?].*)?$' "$output_dir/all_urls.txt" >"$output_dir/urls/jsp_urls.txt"
        grep -iE '\.json([/?].*)?$' "$output_dir/all_urls.txt" >"$output_dir/urls/json_urls.txt"
        grep -iE '\.txt([/?].*)?$' "$output_dir/all_urls.txt" >"$output_dir/urls/txt_urls.txt"
        grep -iE '\.html([/?].*)?$' "$output_dir/all_urls.txt" >"$output_dir/urls/html_urls.txt"
        grep -iE '\.css([/?].*)?$' "$output_dir/all_urls.txt" >"$output_dir/urls/css_urls.txt"
        # Keyword filters (auth-related endpoints)
        grep -iE 'login|signin|signup|auth|register|logout|password|reset' "$output_dir/all_urls.txt" >"$output_dir/urls/auth_urls.txt"

        log "Filtering URLs by file extensions completed. Results saved in $output_dir."
    }

    # Run all modules
    log "Starting Recon Framework..."
    run_passive_recon
    run_active_recon
    run_ffuf_passive
    run_httpx1
    run_dirsearch_batch
    run_katana
    run_waybackurls
    run_gau
    combine_all_urls
    run_httpx2
    filter_urls

    log "Recon automation completed. Results saved in $output_dir."
}

#run_recon_main() {
#     while getopts ":l:" opt; do
#         case ${opt} in
#         l)
#             input_file="$OPTARG"
#             if [[ ! -f "$input_file" ]]; then
#                 echo "Error: File '$input_file' not found."
#                 exit 1
#             fi

#             while read -r domain || [[ -n "$domain" ]]; do
#                 [[ -z "$domain" ]] && continue
#                 run_recon_for_domain "$domain"
#             done <"$input_file"
#             ;;
#         \?)
#             echo "Invalid option: -$OPTARG"
#             exit 1
#             ;;
#         :)
#             echo "Option -$OPTARG requires an argument."
#             exit 1
#             ;;
#         esac
#     done
# }

usage() {
    echo -e "\nUsage: $0 -d <domain> | -l <domain_list.txt>"
    exit 1
}

# Argument parsing
while getopts ":d:l:" opt; do
    case $opt in
    d)
        domain="$OPTARG"
        run_recon_for_domain "$domain"
        ;;
    l)
        list="$OPTARG"
        if [[ ! -f "$list" ]]; then
            echo "Error: File '$list' not found."
            exit 1
        fi
        mapfile -t domains <"$list"
        for domain in "${domains[@]}"; do
            [[ -z "$domain" ]] && continue
            run_recon_for_domain "$domain"
        done

        ;;
    \?)
        echo "Invalid option: -$OPTARG"
        exit 1
        ;;
    :)
        echo "Option -$OPTARG requires an argument."
        exit 1
        ;;
    esac
done

# Check if no options provided
if [[ -z "$domain" && -z "$list" ]]; then
    usage
fi

# Check dependencies
for tool in subfinder assetfinder httpx; do
    if ! command -v $tool &>/dev/null; then
        echo -e "${RED}Error:${RESET} '$tool' not found. Please install it."
        exit 1
    fi
done
