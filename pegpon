#!/bin/bash

# Exit on error, undefined vars, and pipe failures
set -euo pipefail

# Colors for output
RED="\e[31m"
GREEN="\e[32m"
YELLOW="\e[33m"
RESET="\e[0m"

# Check dependencies early
REQUIRED_TOOLS=(subfinder assetfinder httpx ffuf jq curl katana waybackurls gau findomain chaos github-subdomains dnsx nuclei)
for tool in "${REQUIRED_TOOLS[@]}"; do do
    if ! command -v "$tool" &>/dev/null; then
        echo -e "${RED}Error:${RESET} '$tool' not found. Please install it."
        exit 1
    fi
done

# ┌─────────────────────────────────────────────┐
# │                                             │
# |         RECON TOOL - v1.0   @M9nx           │
# │           Subdomain & Web Recon             |
# └─────────────────────────────────────────────┘
banner() {
    cat <<"EOF"

██████╗ ███████╗ ██████╗ ██████╗  ██████╗ ███╗   ██╗
██╔══██╗██╔════╝██╔════╝ ██╔══██╗██╔═══██╗████╗  ██║
██████╔╝█████╗  ██║  ███╗██████╔╝██║   ██║██╔██╗ ██║
██╔═══╝ ██╔══╝  ██║   ██║██╔═══╝ ██║   ██║██║╚██╗██║
██║     ███████╗╚██████╔╝██║     ╚██████╔╝██║ ╚████║
╚═╝     ╚══════╝ ╚═════╝ ╚═╝      ╚═════╝ ╚═╝  ╚═══╝

        [ PEGPON - v1.1   @M9nx ]
EOF
}
# Entry point
banner
run_recon_for_domain() {
    domain="$1"
    echo "Running recon for: $domain"
    output_dir="recon/$domain"
    mkdir -p "$output_dir"
    log_file="$output_dir/recon.log"

    log() {
        echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$log_file"
    }

    # Passive Subdomain Enumeration
    run_passive_recon() {
        log "Running Passive Recon..."

        # Export Chaos API Key
        export CHAOS_KEY="d34657f0-33a3-4645-be5c-e3215513852c"
        export GITHUB_TOKEN="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"
        export SECURITYTRAILS_KEY="30HDfBsd87UmJGh78Hd40-gNQwCKI10S"  #-->new
        export NETLAS_API_KEY="4lmdmURzX8eRsJjiKplUdRF3WKRProY8"      #-->new
        export ZOOMEYE_API_KEY="48A6817D-FCC3-f3535-1df3-cab4159b7fd" #-->new
        export SHODAN_API_KEY="${SHODAN_API_KEY:-}"                   # Set via env

        # Make sure output dir exists
        mkdir -p "$output_dir"

        # Subfinder
        log "Using Subfinder..."
        subfinder -d $domain -o $output_dir/subfinder.txt

        # Assetfinder
        log "Using Assetfinder..."
        assetfinder --subs-only $domain >$output_dir/assetfinder.txt

        # Sublist3r
        log "Using Sublist3r..."
        sublist3r -d $domain -o $output_dir/sublist3r.txt

        # crt.sh via curl
        log "Using crt.sh..."
        curl -s "https://crt.sh/?q=%25.$domain&output=json" |
            jq -r '.[].name_value' |
            sed 's/\*\.//g' |
            grep -iE "([a-z0-9_\-]+\.)+$domain$" |
            sort -u >$output_dir/crtsh.txt

        # ShrewdEye scraping (static HTML parsing)  -->new
        log "Using ShrewdEye..."
        curl -s "https://shrewdeye.app/?q=$domain" |
            grep -oP '([a-zA-Z0-9_-]+\.)+'"$domain" |
            sort -u >"$output_dir/shrewdeye.txt"

        # SecurityTrails API (requires SECURITYTRAILS_KEY)  -->new
        if [[ -z "$SECURITYTRAILS_KEY" ]]; then
            log "Skipping SecurityTrails: SECURITYTRAILS_KEY not set."
        else
            log "Using SecurityTrails API..."
            curl -s -H "APIKEY: $SECURITYTRAILS_KEY" \
                "https://api.securitytrails.com/v1/domain/$domain/subdomains" |
                jq -r '.subdomains[]' |
                sed "s|$|.$domain|" |
                sort -u >"$output_dir/securitytrails.txt"
        fi

        # Findomain
        log "Using Findomain..."
        findomain --target $domain --quiet --output || true
        [[ -f "$domain.txt" ]] && mv "$domain.txt" "$output_dir/findomain.txt" || touch "$output_dir/findomain.txt"

        # Chaos
        log "Using Chaos dataset..."
        chaos -d $domain -o $output_dir/chaos.txt

        # GitHub Subdomains (requires GitHub token set in env var)
        if [ ! -z "$GITHUB_TOKEN" ]; then
            log "Using GitHub-subdomains..."
            github-subdomains -d $domain -t $GITHUB_TOKEN -o $output_dir/github.txt
        else
            log "Skipping GitHub-subdomains: GITHUB_TOKEN not set."
        fi

        # Netlas.io (requires API key) #-->new
        if [[ -n "$NETLAS_API_KEY" ]]; then
            log "Using Netlas.io..."
            curl -s "https://app.netlas.io/api/domains?q=domain:$domain" \
                -H "X-API-Key: $NETLAS_API_KEY" |
                jq -r '.items[].name' |
                grep "$domain" |
                sort -u >"$output_dir/netlas.txt"
        else
            log "Skipping Netlas.io: API key not set."
        fi

        # Bufferover.run #-->new
        log "Using Bufferover..."
        curl -s "https://dns.bufferover.run/dns?q=.$domain" |
            jq -r '.FDNS_A[]?' |
            cut -d',' -f2 |
            grep "$domain" |
            sort -u >"$output_dir/bufferover.txt"

        # Zoomeye (requires API key) #-->new
        if [[ -n "$ZOOMEYE_API_KEY" ]]; then
            log "Using Zoomeye..."
            curl -s -H "API-KEY: $ZOOMEYE_API_KEY" \
                "https://api.zoomeye.org/domain/search?q=$domain" |
                jq -r '.matches[].name' |
                grep "$domain" |
                sort -u >"$output_dir/zoomeye.txt"
        else
            log "Skipping Zoomeye: API key not set."
        fi

        # LeakIX #-->new
        log "Using LeakIX..."
        curl -s "https://leakix.net/domain/$domain" |
            grep -oP '([a-zA-Z0-9_-]+\.)+'"$domain" |
            sort -u >"$output_dir/leakix.txt"

        # Combine & deduplicate all results into passive_subdomains.txt
        log "Combining and sorting results..."
        tmp_combined="$output_dir/tmp_combined_passive.txt"
        cat $output_dir/*.txt 2>/dev/null | sort -u >"$tmp_combined"
        mv "$tmp_combined" "$output_dir/passive_subdomains.txt"

        # Clean up all temp files except passive_subdomains.txt
        log "Cleaning up temporary files..."
        find $output_dir -type f -name '*.txt' ! -name 'passive_subdomains.txt' -delete

        # Wildcard detection
        log "Checking for wildcard DNS..."
        wildcard_ip=$(dig +short "randomnonexistent$(date +%s).$domain" 2>/dev/null | head -1 || true)
        if [[ -n "$wildcard_ip" ]]; then
            log "WARNING: Wildcard DNS detected ($wildcard_ip) - filtering results..."
            grep -v "$wildcard_ip" "$output_dir/passive_subdomains.txt" > "$output_dir/passive_filtered.txt" || true
            mv "$output_dir/passive_filtered.txt" "$output_dir/passive_subdomains.txt"
        fi

        # DNS Resolution Validation
        log "Validating subdomains via DNS resolution..."
        cat "$output_dir/passive_subdomains.txt" | dnsx -silent -o "$output_dir/resolved_subdomains.txt" || true
        if [[ -s "$output_dir/resolved_subdomains.txt" ]]; then
            mv "$output_dir/resolved_subdomains.txt" "$output_dir/passive_subdomains.txt"
        fi

        log "Passive recon completed. Results saved to $output_dir/passive_subdomains.txt"
    }

    # Active Subdomain Enumeration via FFUF Subdomain Fuzzing
    run_active_recon() {
        log "Running FFUF Subdomain Fuzzing..."
        wordlist="/usr/share/dirb/wordlists/common.txt"

        # Create output directory if not exists
        mkdir -p "$output_dir"

        # Temporary file to collect all subs
        tmp_file="$output_dir/tmp_active_subdomains.txt"
        >"$tmp_file"

        # Level 1
        log "[Level 1] Fuzzing FUZZ.$domain"
        ffuf -w $wordlist -u "https://FUZZ.$domain" -H "Host: FUZZ.$domain" -fs 0 -s -rate 100 -timeout 10 -o "$output_dir/ffuf_lvl1.json" -of json
        level1_results=$(jq -r '.results[].input.FUZZ' "$output_dir/ffuf_lvl1.json")

        if [[ -n "$level1_results" ]]; then
            echo "$level1_results" | sed "s|$|.$domain|" >>"$tmp_file"
            rm "$output_dir/ffuf_lvl1.json"

            # Level 2
            for sub1 in $level1_results; do
                log "[Level 2] Fuzzing FUZZ.$sub1.$domain"
                ffuf -w $wordlist -u "https://FUZZ.$sub1.$domain" -H "Host: FUZZ.$sub1.$domain" -fs 0 -s -rate 100 -timeout 10 -o "$output_dir/ffuf_lvl2_${sub1}.json" -of json
                level2_results=$(jq -r '.results[].input.FUZZ' "$output_dir/ffuf_lvl2_${sub1}.json")

                if [[ -n "$level2_results" ]]; then
                    echo "$level2_results" | sed "s|$|.$sub1.$domain|" >>"$tmp_file"

                    # Level 3
                    for sub2 in $level2_results; do
                        log "[Level 3] Fuzzing FUZZ.$sub2.$sub1.$domain"
                        ffuf -w $wordlist -u "https://FUZZ.$sub2.$sub1.$domain" -H "Host: FUZZ.$sub2.$sub1.$domain" -fs 0 -s -rate 100 -timeout 10 -o "$output_dir/ffuf_lvl3_${sub2}_${sub1}.json" -of json
                        level3_results=$(jq -r '.results[].input.FUZZ' "$output_dir/ffuf_lvl3_${sub2}_${sub1}.json")

                        if [[ -n "$level3_results" ]]; then
                            echo "$level3_results" | sed "s|$|.$sub2.$sub1.$domain|" >>"$tmp_file"
                        fi
                        rm "$output_dir/ffuf_lvl3_${sub2}_${sub1}.json"
                    done
                fi
                rm "$output_dir/ffuf_lvl2_${sub1}.json"
            done
        else
            log "No results found at Level 1. Halting recursion."
            rm "$output_dir/ffuf_lvl1.json"
        fi

        # Remove duplicates and save final results
        sort -u "$tmp_file" >"$output_dir/active_subdomains.txt"

        # Clean up temp file
        rm "$tmp_file"

        log "Active recon completed. Results saved to $output_dir/active_subdomains.txt"
    }

    # Fuzzing Passive Subdomins with ffuf
    run_ffuf_passive() {
        log "Fuzzing passive subdomains with ffuf..."
        wordlist="/usr/share/dirb/wordlists/common.txt"
        passive_file="$output_dir/passive_subdomains.txt"

        mkdir -p "$output_dir"

        tmp_file="$output_dir/tmp_active_passive.txt"
        >"$tmp_file"

        # Loop over each passive subdomain line by line
        while read -r base; do
            current_subs="$base"

            for level in {1..3}; do
                next_subs=""

                # Loop over current subs (split by space)
                for sub in $current_subs; do
                    log "[Level $level] Fuzzing FUZZ.$sub"

                    ffuf -w "$wordlist" -u "https://FUZZ.$sub" -H "Host: FUZZ.$sub" -fs 0 -s -rate 100 -timeout 10 -o "$output_dir/ffuf_${sub}_lvl${level}.json" -of json
                    fuzz_results=$(jq -r '.results[].input.FUZZ' "$output_dir/ffuf_${sub}_lvl${level}.json")

                    if [[ -n "$fuzz_results" ]]; then
                        # Add discovered subs to temp file with full path
                        echo "$fuzz_results" | sed "s|$|.$sub|" >>"$tmp_file"

                        # Prepare next level subs, space-separated
                        next_subs="$next_subs $(echo "$fuzz_results" | sed "s|$|.$sub|")"
                    fi

                    rm "$output_dir/ffuf_${sub}_lvl${level}.json"
                done

                # Update current_subs for the next level (space-separated list)
                current_subs="$next_subs"
            done
        done <"$passive_file"

        # Sort, remove duplicates, and save final result
        sort -u "$tmp_file" >"$output_dir/active_passive_subdomains.txt"
        rm "$tmp_file"

        log "Passive fuzzing completed. Results saved to $output_dir/active_passive_subdomains.txt"
    }

    # HTTP Probing 1
    run_httpx1() {
        log "Probing live subdomains with Httpx..."

        combined_file="$output_dir/combined_subdomains.txt"
        cat "$output_dir/active_subdomains.txt" "$output_dir/passive_subdomains.txt" "$output_dir/active_passive_subdomains.txt" 2>/dev/null | sort -u >"$combined_file"

        cat "$combined_file" | httpx -mc 200,201,301,302,307,308,401,403,405 -fc 404,502,503,504 -timeout 10 -o "$output_dir/final_live_subdomains.txt" || true

        rm -f "$combined_file"

        log "Httpx probing completed. Results saved to $output_dir/final_live_subdomains.txt"
    }

    #run_dirsearch_batch
    run_dirsearch_batch() {
        log "Running Dirsearch on all live subdomains..."

        input_file="$output_dir/final_live_subdomains.txt"
        output_file="$output_dir/dirsearch_valid_urls.txt"

        if [[ ! -f "$input_file" ]]; then
            echo -e "${RED}Error:${RESET} $input_file not found."
            return
        fi

        dirsearch_exts="conf,config,bak,backup,swp,old,db,sql,asp,aspx,asp~,py,py~,rb,rb~,php,php~,bkp,cache,cgi,csv,html,inc,jar,js,json,jsp,jsp~,lock,log,rar,sql.gz,sql.tar.gz,sql~,swp~,tar,tar.bz2,tar.gz,txt,wadl,zip,xml"

        python3 dirsearch/dirsearch.py \
            -l "$input_file" \
            -e "$dirsearch_exts" \
            --plain-text-report "$output_file"

        log "Dirsearch scan completed. Results saved to $output_file"
    }

    # Crawling with Katana
    run_katana() {
        log "Running Katana for web crawling..."
        katana -list "$output_dir/final_live_subdomains.txt" -depth 3 -o "$output_dir/katana_urls.txt"
    }

    # Extract URLs from Wayback Machine
    run_waybackurls() {
        log "Extracting URLs from Wayback Machine..."
        cat "$output_dir/final_live_subdomains.txt" | waybackurls >"$output_dir/wayback_urls.txt"
    }

    # Run GAU to fetch URLs
    run_gau() {
        log "Running GAU for historical URL fetching..."
        combined_output="$output_dir/gau_all_urls.txt"
        >"$combined_output"

        while read -r subdomain; do
            log "Fetching URLs for $subdomain with GAU..."
            gau "$subdomain" >>"$combined_output"
        done <"$output_dir/final_live_subdomains.txt"

        sort -u "$combined_output" -o "$combined_output"

        log "GAU completed. All URLs saved to $combined_output"
    }

    # Combine all URLs into all_urls.txt
    combine_all_urls() {
        log "Combining Katana, Waybackurls, and GAU outputs..."
        cat "$output_dir/katana_urls.txt" \
            "$output_dir/wayback_urls.txt" \
            "$output_dir/gau_all_urls.txt" |
            sort -u >"$output_dir/all_urls.txt"

        log "All URLs combined into $output_dir/all_urls.txt"
    }

    # HTTP Probing 2
    run_httpx2() {
        log "Probing all_urls.txt with Httpx..."

        cat "$output_dir/all_urls.txt" | httpx -mc 200,201,301,302,307,308,401,403,405 -fc 404,502,503,504 -timeout 10 -o "$output_dir/final_all_urls.txt" || true

        log "Httpx probing completed. Results saved to $output_dir/final_all_urls.txt"
    }

    # Filtering URLs by extensions
    filter_urls() {
        log "Filtering URLs by file extensions..."

        # Create the directory if it doesn't exist
        mkdir -p "$output_dir/urls"

        grep -iE '\.js([/?].*)?$' "$output_dir/all_urls.txt" >"$output_dir/urls/js_urls.txt" || true
        grep -iE '\.php([/?].*)?$' "$output_dir/all_urls.txt" >"$output_dir/urls/php_urls.txt" || true
        grep -iE '\.aspx([/?].*)?$' "$output_dir/all_urls.txt" >"$output_dir/urls/aspx_urls.txt" || true
        grep -iE '\.jsp([/?].*)?$' "$output_dir/all_urls.txt" >"$output_dir/urls/jsp_urls.txt" || true
        grep -iE '\.json([/?].*)?$' "$output_dir/all_urls.txt" >"$output_dir/urls/json_urls.txt" || true
        grep -iE '\.txt([/?].*)?$' "$output_dir/all_urls.txt" >"$output_dir/urls/txt_urls.txt" || true
        grep -iE '\.html([/?].*)?$' "$output_dir/all_urls.txt" >"$output_dir/urls/html_urls.txt" || true
        grep -iE '\.css([/?].*)?$' "$output_dir/all_urls.txt" >"$output_dir/urls/css_urls.txt" || true
        # Keyword filters (auth-related endpoints)
        grep -iE 'login|signin|signup|auth|register|logout|password|reset' "$output_dir/all_urls.txt" >"$output_dir/urls/auth_urls.txt" || true

        log "Filtering URLs by file extensions completed. Results saved in $output_dir."
    }

    # Shodan Hunting Module
    run_shodan_recon() {
        log "Running Shodan Recon..."
        
        # API Key check
        if [[ -z "${SHODAN_API_KEY:-}" ]]; then
            log "Skipping Shodan: SHODAN_API_KEY not set."
            return
        fi
        
        mkdir -p "$output_dir/shodan"
        
        # 1. Domain search - find subdomains via Shodan DNS
        log "Searching Shodan for domain subdomains..."
        curl -s "https://api.shodan.io/dns/domain/$domain?key=$SHODAN_API_KEY" | \
            jq -r '.subdomains[]?' 2>/dev/null | sed "s|$|.$domain|" > "$output_dir/shodan/subdomains.txt" || true
        
        # 2. Get IPs for all discovered subdomains
        log "Resolving IPs for Shodan host lookup..."
        cat "$output_dir/passive_subdomains.txt" "$output_dir/shodan/subdomains.txt" 2>/dev/null | \
            sort -u | dnsx -silent -a -resp-only 2>/dev/null | \
            sort -u > "$output_dir/shodan/target_ips.txt" || true
        
        # 3. Query each IP for detailed host info (with rate limiting)
        log "Querying Shodan for host details..."
        > "$output_dir/shodan/host_details.jsonl"
        while read -r ip; do
            [[ -z "$ip" ]] && continue
            curl -s "https://api.shodan.io/shodan/host/$ip?key=$SHODAN_API_KEY" >> "$output_dir/shodan/host_details.jsonl"
            echo "" >> "$output_dir/shodan/host_details.jsonl"
            sleep 1  # Rate limiting to avoid API blocks
        done < "$output_dir/shodan/target_ips.txt"
        
        # 4. Extract open ports
        log "Extracting open ports..."
        jq -r 'select(.ip_str and .ports) | .ip_str as $ip | .ports[] | "\($ip):\(.)"' \
            "$output_dir/shodan/host_details.jsonl" 2>/dev/null | \
            sort -u > "$output_dir/shodan/open_ports.txt" || true
        
        # 5. Extract services/banners
        log "Extracting services and banners..."
        jq -r 'select(.data) | .data[] | "\(.ip_str):\(.port) | \(.transport // "tcp") | \(.product // "unknown") \(.version // "") | \(.info // "")"' \
            "$output_dir/shodan/host_details.jsonl" 2>/dev/null | \
            sort -u > "$output_dir/shodan/services.txt" || true
        
        # 6. Extract CVEs/Vulnerabilities
        log "Extracting vulnerabilities..."
        jq -r 'select(.vulns) | .ip_str as $ip | .vulns | keys[] | "\($ip),\(.)"' \
            "$output_dir/shodan/host_details.jsonl" 2>/dev/null | \
            sort -u > "$output_dir/shodan/cves.txt" || true
        
        # 7. Extract SSL certificate CNs (potential new subdomains)
        log "Extracting SSL certificate data..."
        jq -r '.data[]? | select(.ssl) | .ssl.cert.subject.CN? // empty' \
            "$output_dir/shodan/host_details.jsonl" 2>/dev/null | \
            grep -v null | grep "$domain" | sort -u > "$output_dir/shodan/ssl_cns.txt" || true
        
        # 8. Extract hostnames from Shodan data
        jq -r '.hostnames[]?' "$output_dir/shodan/host_details.jsonl" 2>/dev/null | \
            grep "$domain" | sort -u > "$output_dir/shodan/hostnames.txt" || true
        
        log "Shodan recon completed. Results in $output_dir/shodan/"
    }

    # Shodan Dork Hunting
    run_shodan_dorks() {
        log "Running Shodan dork searches..."
        
        if [[ -z "${SHODAN_API_KEY:-}" ]]; then
            log "Skipping Shodan dorks: SHODAN_API_KEY not set."
            return
        fi
        
        mkdir -p "$output_dir/shodan"
        
        # Define dorks
        dorks=(
            "ssl:$domain"
            "hostname:$domain"
            "http.title:$domain"
            "ssl.cert.subject.cn:$domain"
            "ssl.cert.issuer.cn:$domain"
        )
        
        > "$output_dir/shodan/dork_results.jsonl"
        
        for dork in "${dorks[@]}"; do
            log "Searching: $dork"
            encoded=$(python3 -c "import urllib.parse; print(urllib.parse.quote('$dork'))")
            curl -s "https://api.shodan.io/shodan/host/search?key=$SHODAN_API_KEY&query=$encoded" >> "$output_dir/shodan/dork_results.jsonl"
            echo "" >> "$output_dir/shodan/dork_results.jsonl"
            sleep 1
        done
        
        # Extract unique IPs from dork results
        jq -r '.matches[]?.ip_str?' "$output_dir/shodan/dork_results.jsonl" 2>/dev/null | \
            sort -u > "$output_dir/shodan/dork_ips.txt" || true
        
        # Extract ports from dork results
        jq -r '.matches[]? | "\(.ip_str):\(.port)"' "$output_dir/shodan/dork_results.jsonl" 2>/dev/null | \
            sort -u >> "$output_dir/shodan/open_ports.txt" || true
        
        sort -u "$output_dir/shodan/open_ports.txt" -o "$output_dir/shodan/open_ports.txt"
        
        log "Shodan dork hunting completed."
    }

    # Generate Nuclei-Ready Targets
    prepare_nuclei_targets() {
        log "Preparing Nuclei-ready targets..."
        
        mkdir -p "$output_dir/nuclei"
        
        # 1. All live URLs (from httpx)
        cat "$output_dir/final_live_subdomains.txt" 2>/dev/null > "$output_dir/nuclei/urls.txt" || true
        cat "$output_dir/final_all_urls.txt" 2>/dev/null >> "$output_dir/nuclei/urls.txt" || true
        sort -u "$output_dir/nuclei/urls.txt" -o "$output_dir/nuclei/urls.txt"
        
        # 2. HTTP/HTTPS targets from Shodan ports
        log "Converting Shodan ports to HTTP targets..."
        > "$output_dir/nuclei/shodan_http_targets.txt"
        
        if [[ -f "$output_dir/shodan/open_ports.txt" ]]; then
            while read -r hostport; do
                [[ -z "$hostport" ]] && continue
                ip=$(echo "$hostport" | cut -d: -f1)
                port=$(echo "$hostport" | cut -d: -f2)
                
                # Common HTTP ports
                case "$port" in
                    80|8080|8000|8008|8081|8888|3000|5000|9000)
                        echo "http://$hostport" >> "$output_dir/nuclei/shodan_http_targets.txt"
                        ;;
                    443|8443|4443|9443)
                        echo "https://$hostport" >> "$output_dir/nuclei/shodan_http_targets.txt"
                        ;;
                    *)
                        # Try both for unknown ports
                        echo "http://$hostport" >> "$output_dir/nuclei/shodan_http_targets.txt"
                        echo "https://$hostport" >> "$output_dir/nuclei/shodan_http_targets.txt"
                        ;;
                esac
            done < "$output_dir/shodan/open_ports.txt"
        fi
        
        sort -u "$output_dir/nuclei/shodan_http_targets.txt" -o "$output_dir/nuclei/shodan_http_targets.txt"
        
        # 3. Probe Shodan targets with httpx for live validation
        log "Probing Shodan HTTP targets..."
        cat "$output_dir/nuclei/shodan_http_targets.txt" | \
            httpx -silent -mc 200,201,301,302,307,308,401,403,405,500 -timeout 10 \
            -o "$output_dir/nuclei/shodan_live.txt" 2>/dev/null || true
        
        # 4. Raw IP:Port targets for non-HTTP nuclei templates (network templates)
        cp "$output_dir/shodan/open_ports.txt" "$output_dir/nuclei/network_targets.txt" 2>/dev/null || true
        
        # 5. Combined master target list for nuclei
        log "Creating master nuclei target list..."
        cat "$output_dir/nuclei/urls.txt" \
            "$output_dir/nuclei/shodan_live.txt" 2>/dev/null | \
            sort -u > "$output_dir/nuclei/all_targets.txt" || true
        
        # 6. CVE-specific targets (IPs with known vulns)
        if [[ -f "$output_dir/shodan/cves.txt" && -s "$output_dir/shodan/cves.txt" ]]; then
            log "Creating CVE-specific target list..."
            cut -d',' -f1 "$output_dir/shodan/cves.txt" | sort -u > "$output_dir/nuclei/cve_ips.txt"
            
            # Create CVE mapping file for reference
            cp "$output_dir/shodan/cves.txt" "$output_dir/nuclei/cve_mapping.csv"
        fi
        
        # 7. Generate nuclei commands
        cat > "$output_dir/nuclei/run_nuclei.sh" << 'NUCLEI_SCRIPT'
#!/bin/bash
# Auto-generated Nuclei scan commands

OUTPUT_DIR="$(dirname "$0")"
RESULTS_DIR="$OUTPUT_DIR/results"
mkdir -p "$RESULTS_DIR"

echo "[*] Running Nuclei scans..."

# Full scan on all HTTP targets
echo "[+] Scanning all HTTP targets..."
nuclei -l "$OUTPUT_DIR/all_targets.txt" -t cves/ -t vulnerabilities/ -t exposures/ -t misconfigurations/ \
    -severity critical,high,medium -o "$RESULTS_DIR/vuln_scan.txt" -silent

# Technology detection
echo "[+] Running technology detection..."
nuclei -l "$OUTPUT_DIR/all_targets.txt" -t technologies/ -o "$RESULTS_DIR/tech_scan.txt" -silent

# Network protocol scans (requires network templates)
if [[ -f "$OUTPUT_DIR/network_targets.txt" ]]; then
    echo "[+] Running network scans..."
    nuclei -l "$OUTPUT_DIR/network_targets.txt" -t network/ -o "$RESULTS_DIR/network_scan.txt" -silent
fi

# CVE-specific scans on vulnerable IPs
if [[ -f "$OUTPUT_DIR/cve_ips.txt" ]]; then
    echo "[+] Running targeted CVE scans on known vulnerable IPs..."
    nuclei -l "$OUTPUT_DIR/cve_ips.txt" -t cves/ -severity critical,high -o "$RESULTS_DIR/cve_targeted.txt" -silent
fi

# Exposed panels and dashboards
echo "[+] Scanning for exposed panels..."
nuclei -l "$OUTPUT_DIR/all_targets.txt" -t exposed-panels/ -t default-logins/ \
    -o "$RESULTS_DIR/panels_scan.txt" -silent

# Takeover checks
echo "[+] Checking for subdomain takeovers..."
nuclei -l "$OUTPUT_DIR/all_targets.txt" -t takeovers/ -o "$RESULTS_DIR/takeover_scan.txt" -silent

echo "[*] Nuclei scans completed. Results in $RESULTS_DIR/"
NUCLEI_SCRIPT
        
        chmod +x "$output_dir/nuclei/run_nuclei.sh"
        
        # Count targets
        total_urls=$(wc -l < "$output_dir/nuclei/all_targets.txt" 2>/dev/null || echo "0")
        total_network=$(wc -l < "$output_dir/nuclei/network_targets.txt" 2>/dev/null || echo "0")
        
        log "Nuclei targets prepared:"
        log "  - HTTP targets: $total_urls"
        log "  - Network targets: $total_network"
        log "  - Run: $output_dir/nuclei/run_nuclei.sh"
    }

    # Run all modules
    log "Starting Recon Framework..."
    run_passive_recon
    run_active_recon
    run_ffuf_passive
    run_httpx1
    run_dirsearch_batch
    run_katana
    run_waybackurls
    run_gau
    combine_all_urls
    run_httpx2
    filter_urls
    run_shodan_recon
    run_shodan_dorks
    prepare_nuclei_targets

    log "Recon automation completed. Results saved in $output_dir."
    log "To run Nuclei scans: bash $output_dir/nuclei/run_nuclei.sh"
}

#run_recon_main() {
#     while getopts ":l:" opt; do
#         case ${opt} in
#         l)
#             input_file="$OPTARG"
#             if [[ ! -f "$input_file" ]]; then
#                 echo "Error: File '$input_file' not found."
#                 exit 1
#             fi

#             while read -r domain || [[ -n "$domain" ]]; do
#                 [[ -z "$domain" ]] && continue
#                 run_recon_for_domain "$domain"
#             done <"$input_file"
#             ;;
#         \?)
#             echo "Invalid option: -$OPTARG"
#             exit 1
#             ;;
#         :)
#             echo "Option -$OPTARG requires an argument."
#             exit 1
#             ;;
#         esac
#     done
# }

usage() {
    echo -e "\nUsage: $0 -d <domain> | -l <domain_list.txt>"
    exit 1
}

# Argument parsing
while getopts ":d:l:" opt; do
    case $opt in
    d)
        domain="$OPTARG"
        run_recon_for_domain "$domain"
        ;;
    l)
        list="$OPTARG"
        if [[ ! -f "$list" ]]; then
            echo "Error: File '$list' not found."
            exit 1
        fi
        mapfile -t domains <"$list"
        for domain in "${domains[@]}"; do
            [[ -z "$domain" ]] && continue
            run_recon_for_domain "$domain"
        done

        ;;
    \?)
        echo "Invalid option: -$OPTARG"
        exit 1
        ;;
    :)
        echo "Option -$OPTARG requires an argument."
        exit 1
        ;;
    esac
done

# Check if no options provided
if [[ -z "${domain:-}" && -z "${list:-}" ]]; then
    usage
fi
